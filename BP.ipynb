{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 理解BP算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "直觉告诉我们：  \n",
    "同一层中权重较大的连接在反向传播误差时分担了更多的误差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![fanxiang](images/反向传播误差.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "对于复杂的非线性问题（包含激活函数），想要让误差最小/得到损失函数最小值，\n",
    "**梯度下降法**是一种很好的方法\n",
    "- 优点：\n",
    "    - 方法本身简单，要求低\n",
    "    - 方法具有弹性，能够容忍不完善数据\n",
    "- 缺点：\n",
    "    - 整体收敛速度不一定最快"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "对于一个损失函数为**平方误差函数**，激活函数为**Sigmoid函数**的单隐层神经网络。\n",
    " \n",
    "其隐藏层与输出层权重的更新值为：$\\frac{\\partial E}{\\partial W_{j,k}} = -(t_k-o_k) \\cdot o_k \\cdot (1-o_k) \\cdot o_j$  \n",
    "其输入层与隐藏层权重的更新值为：$\\frac{\\partial E}{\\partial W_{i,j}} = - e_j \\cdot o_j \\cdot (1-o_j) \\cdot o_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 使用单隐层神经网络完成手写数字识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import scipy.special\n",
    "\n",
    "class neuralNetwork:\n",
    "    # 定义神经网络\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "        \n",
    "        # w11 w21\n",
    "        # w12 w22\n",
    "        self.wih = numpy.random.normal(0.0, pow(self.inodes, -0.5), (self.hnodes, self.inodes))\n",
    "        self.who = numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.onodes, self.hnodes))\n",
    "\n",
    "        self.lr = learningrate\n",
    "        \n",
    "        # sigmoid\n",
    "        self.activation_function = lambda x: scipy.special.expit(x)\n",
    "        \n",
    "\n",
    "    def train(self, inputs_list, targets_list):\n",
    "        # 将输入转换为二维\n",
    "        inputs = numpy.array(inputs_list, ndmin=2).T\n",
    "        targets = numpy.array(targets_list, ndmin=2).T\n",
    "        \n",
    "        \n",
    "        hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "        \n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        \n",
    "        final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "        \n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        # 输出层误差为 target - 输出\n",
    "        output_errors = targets - final_outputs\n",
    "        # 隐层误差为 隐层输出层权重*输出层误差\n",
    "        hidden_errors = numpy.dot(self.who.T, output_errors) \n",
    "        \n",
    "        # 更新两层权重，根据公式\n",
    "        self.who += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs))\n",
    "        self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), numpy.transpose(inputs))\n",
    "        \n",
    "\n",
    "    \n",
    "    def query(self, inputs_list):\n",
    "        inputs = numpy.array(inputs_list, ndmin=2).T\n",
    "        \n",
    "        hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        final_inputs = numpy.dot(self.who, hidden_outputs)\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# 初始化节点数量和学习率\n",
    "input_nodes = 784\n",
    "hidden_nodes = 200\n",
    "output_nodes = 10\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "n = neuralNetwork(input_nodes,hidden_nodes,output_nodes, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# 加载数据集和测试集\n",
    "with open(\"mnist_train.csv\", 'r')as f:\n",
    "    training_data_list = f.readlines()\n",
    "with open(\"mnist_test.csv\", 'r') as f2:\n",
    "    test_data_list = f2.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "\n",
    "for e in range(epochs):\n",
    "    for record in training_data_list:\n",
    "        all_values = record.split(',')\n",
    "        inputs = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
    "        targets = numpy.zeros(output_nodes) + 0.01\n",
    "        targets[int(all_values[0])] = 0.99\n",
    "        n.train(inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# 计算准确率，使用得分板\n",
    "scorecard = []\n",
    "for record in test_data_list:\n",
    "    all_values = record.split(',')\n",
    "    correct_label = int(all_values[0])\n",
    "    inputs = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
    "    outputs = n.query(inputs)\n",
    "    label = numpy.argmax(outputs)\n",
    "    if (label == correct_label):\n",
    "        scorecard.append(1)\n",
    "    else:\n",
    "        scorecard.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performance = 0.9766\n"
     ]
    }
   ],
   "source": [
    "scorecard_array = numpy.asarray(scorecard)\n",
    "print (f\"performance = {scorecard_array.sum() / scorecard_array.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 自动求导\n",
    "- 自动求导计算一个函数在指定值上的导数\n",
    "- 与符号求导、数值求导不同"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- 符号求导\n",
    "![fuhao](images/符号求导.png)\n",
    "- 数值求导\n",
    "![shuzhi](images/数值求导.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 计算图\n",
    "- 将公式分解成最小操作（元操作、操作子）\n",
    "- 将计算表示成一个无环图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![DAG](images/无环图.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- 链式法则: $\\frac{\\partial y}{\\partial x}=\\frac{\\partial y}{\\partial u_n} \\frac{\\partial u_n}{\\partial u_{n-1}} \\ldots \\frac{\\partial u_2}{\\partial u_1} \\frac{\\partial u_1}{\\partial x}$\n",
    "-正向累积 $$\\frac{\\partial y}{\\partial x}=\\frac{\\partial y}{\\partial u_n}\\left(\\frac{\\partial u_n}{\\partial u_{n-1}}\\left(\\ldots\\left(\\frac{\\partial u_2}{\\partial u_1} \\frac{\\partial u_1}{\\partial x}\\right)\\right)\\right)$$\n",
    "-反向累积、又称反向传递\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x}=\\left(\\left(\\left(\\frac{\\partial y}{\\partial u_n} \\frac{\\partial u_n}{\\partial u_{n-1}}\\right) \\ldots\\right) \\frac{\\partial u_2}{\\partial u_1}\\right) \\frac{\\partial u_1}{\\partial x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 计算单个变量梯度的复杂度\n",
    "- 时间复杂度\n",
    "    - 正向反向均为$O(n)$\n",
    "- 空间复杂度\n",
    "    - 正向$O(1)$\n",
    "    - 反向$O(n)$用于存储正向计算的全部结果，**需要计算多个变量的梯度时效果更优**🌟\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 使用pytorch自动求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class MNISTDataSet(Dataset):\n",
    "    def __init__(self,path):\n",
    "        with open(path,'r') as f:\n",
    "            csv = pd.read_csv(path,header=None)\n",
    "        targets = csv.values[:,0]\n",
    "        self.features = csv.values[:,1:]/255.0*0.99 + 0.01\n",
    "\n",
    "        self.labels = numpy.zeros((targets.shape[0],10)) + 0.01\n",
    "        for i in range(len(targets)):\n",
    "            self.labels[i,int(targets[i])] = 0.99\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.labels[idx],self.features[idx]\n",
    "        \n",
    "train_set = MNISTDataSet('mnist_train.csv')\n",
    "test_set = MNISTDataSet('mnist_test.csv')\n",
    "train_loader = DataLoader(train_set,batch_size=64,shuffle=True)\n",
    "test_loader = DataLoader(test_set,batch_size=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "class SingleHiddenNerualNetwork(nn.Module):\n",
    "    def __init__(self,input_num,hidden_num,output_num):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(in_features=input_num,out_features=hidden_num,bias=True)\n",
    "        self.layer2 = nn.Linear(in_features=hidden_num,out_features=output_num,bias=True)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.layer1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.layer2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self,x):\n",
    "        x = torch.sigmoid(self.layer2(torch.sigmoid(self.layer1(x))))\n",
    "        y = torch.argmax(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "\n",
    "net = SingleHiddenNerualNetwork(784,200,10)\n",
    "net = net.double()\n",
    "net.apply(weights_init)\n",
    "\n",
    "epochs = 200\n",
    "lr = 0.1\n",
    "loss = nn.MSELoss()\n",
    "updater = torch.optim.SGD(net.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0 loss:0.08034828241029349\n",
      "epoch1 loss:0.07670363993730017\n",
      "epoch2 loss:0.067474609922055\n",
      "epoch3 loss:0.06000419883749577\n",
      "epoch4 loss:0.04718294404907688\n",
      "epoch5 loss:0.03745918295145245\n",
      "epoch6 loss:0.0393590795468021\n",
      "epoch7 loss:0.03784484748513417\n",
      "epoch8 loss:0.041346607476078864\n",
      "epoch9 loss:0.030116500240849952\n",
      "epoch10 loss:0.028333975697377056\n",
      "epoch11 loss:0.025343488888708687\n",
      "epoch12 loss:0.022716151022046455\n",
      "epoch13 loss:0.02659029279208825\n",
      "epoch14 loss:0.035229157848461236\n",
      "epoch15 loss:0.030771429809598388\n",
      "epoch16 loss:0.022124340451695833\n",
      "epoch17 loss:0.018623662176440965\n",
      "epoch18 loss:0.02282241545428494\n",
      "epoch19 loss:0.027965213975814186\n",
      "epoch20 loss:0.03331219396558747\n",
      "epoch21 loss:0.03331885266981581\n",
      "epoch22 loss:0.03032288276695269\n",
      "epoch23 loss:0.023403952431129773\n",
      "epoch24 loss:0.019845140228108328\n",
      "epoch25 loss:0.01556962601845606\n",
      "epoch26 loss:0.021563081489569293\n",
      "epoch27 loss:0.024676564378801686\n",
      "epoch28 loss:0.023322325628249742\n",
      "epoch29 loss:0.017192547031382586\n",
      "epoch30 loss:0.02009533271536083\n",
      "epoch31 loss:0.01977679745358478\n",
      "epoch32 loss:0.020692436350981634\n",
      "epoch33 loss:0.015096958043233206\n",
      "epoch34 loss:0.02103697720091536\n",
      "epoch35 loss:0.015490085135462345\n",
      "epoch36 loss:0.023981016044668655\n",
      "epoch37 loss:0.024010951928532788\n",
      "epoch38 loss:0.027594991604223018\n",
      "epoch39 loss:0.02736569630445726\n",
      "epoch40 loss:0.011280586818377493\n",
      "epoch41 loss:0.022456403659320424\n",
      "epoch42 loss:0.01565264434154439\n",
      "epoch43 loss:0.021982904226375347\n",
      "epoch44 loss:0.010922058622196303\n",
      "epoch45 loss:0.023013070638369625\n",
      "epoch46 loss:0.021985205807800275\n",
      "epoch47 loss:0.012477844829521006\n",
      "epoch48 loss:0.012278432167839373\n",
      "epoch49 loss:0.020759895040547198\n",
      "epoch50 loss:0.02548638361043254\n",
      "epoch51 loss:0.022838427736621945\n",
      "epoch52 loss:0.019045916376398114\n",
      "epoch53 loss:0.016575743205668382\n",
      "epoch54 loss:0.01506744065271352\n",
      "epoch55 loss:0.02036025979026145\n",
      "epoch56 loss:0.014929430500395913\n",
      "epoch57 loss:0.01418370991388727\n",
      "epoch58 loss:0.013352853191530867\n",
      "epoch59 loss:0.016310882613314148\n",
      "epoch60 loss:0.020344534281571104\n",
      "epoch61 loss:0.01215868038454752\n",
      "epoch62 loss:0.006174299217747753\n",
      "epoch63 loss:0.019268353878722007\n",
      "epoch64 loss:0.01114682755859536\n",
      "epoch65 loss:0.014029049894527277\n",
      "epoch66 loss:0.019137289668591413\n",
      "epoch67 loss:0.0194062592352361\n",
      "epoch68 loss:0.01782157617590326\n",
      "epoch69 loss:0.01780855280165039\n",
      "epoch70 loss:0.01705105801708815\n",
      "epoch71 loss:0.01923885421023868\n",
      "epoch72 loss:0.011208921727038917\n",
      "epoch73 loss:0.015817113117652888\n",
      "epoch74 loss:0.007804444269711228\n",
      "epoch75 loss:0.018153706533032637\n",
      "epoch76 loss:0.013111384802057393\n",
      "epoch77 loss:0.015216850708573915\n",
      "epoch78 loss:0.011037742369815418\n",
      "epoch79 loss:0.01802125443801964\n",
      "epoch80 loss:0.013779779660976632\n",
      "epoch81 loss:0.007646403477207062\n",
      "epoch82 loss:0.011705401720456301\n",
      "epoch83 loss:0.006392569323288409\n",
      "epoch84 loss:0.014501065862425836\n",
      "epoch85 loss:0.015414552513362361\n",
      "epoch86 loss:0.028247875599164807\n",
      "epoch87 loss:0.010438847344833288\n",
      "epoch88 loss:0.02333366611488607\n",
      "epoch89 loss:0.012086630476927086\n",
      "epoch90 loss:0.01592456396647628\n",
      "epoch91 loss:0.010538924781211776\n",
      "epoch92 loss:0.012036842848520982\n",
      "epoch93 loss:0.01619159648379408\n",
      "epoch94 loss:0.024329448855756018\n",
      "epoch95 loss:0.014827439869332184\n",
      "epoch96 loss:0.022769363146696996\n",
      "epoch97 loss:0.019175068154022267\n",
      "epoch98 loss:0.008637068978858587\n",
      "epoch99 loss:0.02510780921662681\n",
      "epoch100 loss:0.018974989929637497\n",
      "epoch101 loss:0.009003778949472952\n",
      "epoch102 loss:0.017458321222163863\n",
      "epoch103 loss:0.021306505775195\n",
      "epoch104 loss:0.008615042578422598\n",
      "epoch105 loss:0.016141091508318665\n",
      "epoch106 loss:0.01648998363218689\n",
      "epoch107 loss:0.007748567957388433\n",
      "epoch108 loss:0.024350964219532895\n",
      "epoch109 loss:0.01757658742706796\n",
      "epoch110 loss:0.01169560613610669\n",
      "epoch111 loss:0.018542145589352698\n",
      "epoch112 loss:0.007218503439346094\n",
      "epoch113 loss:0.005557984241511933\n",
      "epoch114 loss:0.0069562651700739025\n",
      "epoch115 loss:0.006769320955196835\n",
      "epoch116 loss:0.016172220121327116\n",
      "epoch117 loss:0.010660544801597813\n",
      "epoch118 loss:0.01413976065738554\n",
      "epoch119 loss:0.013237223020185433\n",
      "epoch120 loss:0.02374323968948606\n",
      "epoch121 loss:0.004486714347551833\n",
      "epoch122 loss:0.013748062192340826\n",
      "epoch123 loss:0.017583100964719135\n",
      "epoch124 loss:0.018652427420563836\n",
      "epoch125 loss:0.02000311053104278\n",
      "epoch126 loss:0.02746198858452716\n",
      "epoch127 loss:0.020074985163903363\n",
      "epoch128 loss:0.01715284988501259\n",
      "epoch129 loss:0.013027401328900049\n",
      "epoch130 loss:0.012242424043193128\n",
      "epoch131 loss:0.006443531905563647\n",
      "epoch132 loss:0.008721504549893237\n",
      "epoch133 loss:0.017014310282547594\n",
      "epoch134 loss:0.019030082452576754\n",
      "epoch135 loss:0.019769872086580827\n",
      "epoch136 loss:0.006773715337320641\n",
      "epoch137 loss:0.011911887706466447\n",
      "epoch138 loss:0.007920627103168099\n",
      "epoch139 loss:0.01080249131812656\n",
      "epoch140 loss:0.015832671051235765\n",
      "epoch141 loss:0.014635879402946897\n",
      "epoch142 loss:0.004609339039840168\n",
      "epoch143 loss:0.005873528236522836\n",
      "epoch144 loss:0.01668900251469322\n",
      "epoch145 loss:0.014039357632768637\n",
      "epoch146 loss:0.008950350073525338\n",
      "epoch147 loss:0.019137986699989628\n",
      "epoch148 loss:0.010898651448096446\n",
      "epoch149 loss:0.013280967173804031\n",
      "epoch150 loss:0.011647435966438614\n",
      "epoch151 loss:0.007378690828872223\n",
      "epoch152 loss:0.004030153952346611\n",
      "epoch153 loss:0.009205510309771979\n",
      "epoch154 loss:0.008399488883698368\n",
      "epoch155 loss:0.012916776118484755\n",
      "epoch156 loss:0.012428156189469422\n",
      "epoch157 loss:0.011955073800699281\n",
      "epoch158 loss:0.01138902997560312\n",
      "epoch159 loss:0.013537494264849997\n",
      "epoch160 loss:0.005826565776356203\n",
      "epoch161 loss:0.006563146736248619\n",
      "epoch162 loss:0.021112813939632567\n",
      "epoch163 loss:0.01516129620585126\n",
      "epoch164 loss:0.021268468919003074\n",
      "epoch165 loss:0.01900467024288282\n",
      "epoch166 loss:0.005833242248690053\n",
      "epoch167 loss:0.017642543139864634\n",
      "epoch168 loss:0.011690478808460955\n",
      "epoch169 loss:0.010304502508508413\n",
      "epoch170 loss:0.010608415257340163\n",
      "epoch171 loss:0.01376715313226351\n",
      "epoch172 loss:0.020413906640042306\n",
      "epoch173 loss:0.01635564309261505\n",
      "epoch174 loss:0.010354542183380017\n",
      "epoch175 loss:0.01941854367183813\n",
      "epoch176 loss:0.015445816048732219\n",
      "epoch177 loss:0.011475439236370856\n",
      "epoch178 loss:0.0041884273573173595\n",
      "epoch179 loss:0.009788277032208375\n",
      "epoch180 loss:0.01340346092817703\n",
      "epoch181 loss:0.008918253562239188\n",
      "epoch182 loss:0.01687966218013205\n",
      "epoch183 loss:0.015862684151457456\n",
      "epoch184 loss:0.010659174821829088\n",
      "epoch185 loss:0.01652205167049118\n",
      "epoch186 loss:0.020323125478753944\n",
      "epoch187 loss:0.021822008850025422\n",
      "epoch188 loss:0.011770013419355218\n",
      "epoch189 loss:0.02129347938708855\n",
      "epoch190 loss:0.016250661605836905\n",
      "epoch191 loss:0.006751858688545084\n",
      "epoch192 loss:0.01197079702411508\n",
      "epoch193 loss:0.02296672108987924\n",
      "epoch194 loss:0.008867225282910205\n",
      "epoch195 loss:0.015917640315128274\n",
      "epoch196 loss:0.01032678629281052\n",
      "epoch197 loss:0.011765246672609139\n",
      "epoch198 loss:0.005577620071182993\n",
      "epoch199 loss:0.016134027981450548\n",
      "cost time:245.71074295043945\n"
     ]
    }
   ],
   "source": [
    "net.train()\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "# 训练开始\n",
    "for i in range(epochs):\n",
    "    for y,x in iter(train_loader):\n",
    "        y_hat = net(x)\n",
    "        l = loss(y_hat,y)\n",
    "        updater.zero_grad()\n",
    "        l.sum().backward()\n",
    "        updater.step()\n",
    "    print(f'epoch{i} loss:{l}')\n",
    "\n",
    "end = time.time()\n",
    "print(f'cost time:{end-start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:0.9345\n"
     ]
    }
   ],
   "source": [
    "# 检查测试集准确率\n",
    "net.eval()\n",
    "metric = [0,0]\n",
    "for test_labels,test_features in iter(test_loader):\n",
    "    test_labels = torch.argmax(test_labels)\n",
    "    label_pre = net.predict(test_features)\n",
    "#     print(label_pre.shape,label_pre)\n",
    "    if label_pre == test_labels:\n",
    "        metric[0]+=1        \n",
    "    metric[1]+=1\n",
    "print(f'acc:{metric[0]/metric[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 小结\n",
    "- 局部最小点\n",
    "- 梯度下降慢、资源占用高\n",
    "- 激活函数灵活"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "幻灯片",
  "kernelspec": {
   "display_name": "Python [conda env:dl] *",
   "language": "python",
   "name": "conda-env-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
